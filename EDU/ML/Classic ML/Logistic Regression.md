19:20     2024/02/07    
Tags: #st
____
### Сигмойда
> используется для предсказания вероятности

![[Pasted image 20240128181949.png]]
![[Pasted image 20240128182002.png]]
т.е формула имеет вид
![[Pasted image 20240128182032.png]]

- По умолчанию порог перевода вероятности в класс - 0.5
  ![[Pasted image 20240128182242.png]]
  >значение y = 0.5 соответствует точке (w,x) = 0 => получается разделяющая поверхность задается уравнением (w,x) = 0, а это линейная гиперплоскость
  
  ![[Pasted image 20240128182449.png]]
> Таким образом, логистическая регрессия является **линейным классификатором.**

## Функция потерь
### MSE 
Задача оптимизации для логистической регрессии будет иметь вид
![[Pasted image 20240207192743.png]]
и если использовать ее, то столкнемся со следующими нюансами:
1. полученная функция будет невупуклая, т.е будет иметь несколько локальных минимумов
   ![[Pasted image 20240207192853.png]]
   2. MSE не очень разумно выдает штрафы (очень маленький штраф за неверное предсказание)
> => MSE не очень хорошо подходит для логистической регрессии

### Log-loss

![[Pasted image 20240128182925.png]]
#### Cons
1. - она выпуклая, то есть у неё один минимум, и градиентный спуск его найдет
2. она корретно штрафует ошибки. Поясним. Пусть объект относится к классу �=+1y=+1. Тогда
    - на абсолютно верном предсказании ошибка 0
    - на абсолютно неверном предсказании 
	    log-loss = −log(≈0)≈+∞ (огромная ошибка)
3. модель, обученная при помощи минимизации log-loss, предсказывает корректные математические вероятности
   
> Основание логарифма обычно берут равным 2, 10 или используют натуральный логарифм.

![[Pasted image 20240128183048.png]]

![[Pasted image 20240128183008.png]]



____
### Zero-Links
[[00 ML]]

____
### Links
[[5М. Логистическая регрессия]]