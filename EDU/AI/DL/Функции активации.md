Tags: #
____
# Сигмоида // Sigmoid
![[Pasted image 20240423184148.png]]
![[Pasted image 20240423184203.png]]
### Features
- Сигмоида хорошо подходит как функция активации на последнем слое нейронной сети в задачах бинарной классификации, так как в совокупности с подходящей функцией потерь для обучения сети (log-loss) на выходе мы получим вероятности классов.
- есть и недостаток, из-за которого ее стараются не использовать в промежуточных слоях сети: при увеличении ∣x∣ значения функции σ(x) слабо изменяются - это означает, что σ′(x)≈0 в этих областях. Близкая к нулю производная может приводить к занулению градиента функции потерь∇Q(w)≈0, а следовательно, обучение сети застопорится (так как сеть, как и некоторые классические ML-модели, обучается градиентным спуском):
  ![[Pasted image 20240423190517.png]]
  #  **Гиперболический тангенс (tanh)**
  ![[Pasted image 20240423190542.png]]
  ![[Pasted image 20240423190603.png]]
  
> связан с сигмойдой формулой и имеет те же свойства
> ![[Pasted image 20240423190700.png]]
# **ReLU**
(rectified linear unit)
![[Pasted image 20240423191125.png]]
![[Pasted image 20240423191136.png]]
### Features
- имеет свойство регуляризации - она зануляет нейроны, принимающие отрицательные значения. Это означает, что в каждый момент времени не все нейроны активны, что снижает переобучение

## LeakyReLU
![[Pasted image 20240423191250.png]]
![[Pasted image 20240423191303.png]]
> вариация Relu для того чтобы убрать недостаток - зануление, а значит отсутствие обучения у некоторых нейронов.
> 



____
### Zero-Links

____
### Links