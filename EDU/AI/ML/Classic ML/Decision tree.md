18:24     2024/02/21    
Tags: #st
____
> ориентированные графы с _корнем_ и несколькими концевыми вершинами (_листьями_).

## Sclearn
```
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(random_state=42)

from sklearn.tree import DecisionTreeRegressor
regressor = DecisionTreeRegressor(random_state=42)
```
### Hyperparams
- _criterion_ - критерий информативности, используемый при построении дерева
- _max_depth_ - максимальная глубина дерева
- _min_samples_split_ - минимальное количество объектов, которые должны находиться в вершине, чтобы её дальше разбивать
- _min_samples_leaf_ - минимальное количество объектов, которое находится в листе (если после разбиения в одной из полученных подгрупп число объектов меньше, чем min_samples_leaf, то разбиение не производится)
- _max_features_ - число признаков, используемых для поиска наилучшего предиката в каждой вершине.


## Особенности
- Очень склонны к переобучению
### Критерий информативности
> Меру неоднородности объектов в вершине

-  **в задаче жесткой классификации критерий информативности H - это ошибка классификации.**
- **в задаче регрессии - дисперсия**

![[Pasted image 20240221184903.png]]
#### **Задачи оптимизации**
**мы ищем такой признак xj​ и такой порог t для него, что разбиение объектов по предикату xj​>t минимизирует взвешенную сумму критериев информативности. Другими словами, решаем задачу**
![[Pasted image 20240221185131.png]]
#### **Information Gain (прирост информации)**

**ищем такой признак xj​ и такой порог t для него, что разбиение объектов по предикату      xj​>t минимизирует взвешенную сумму критериев информативности. Другими словами, решаем задачу**
![[Pasted image 20240221185306.png]]

### Энтропия
> - Если при построении дерева используется энтропийный критерий, то такой алгоритм в литературе называется **ID3 (Iterative Dichotomiser 3)**
![[Pasted image 20240221190844.png]]

где pk​ - доля объектов k-го класса в вершине,  а K - число классов в задаче.
![[Pasted image 20240221190951.png]]

### Критерий Джини
> - Если же используется критерий Джини, то алгоритм называется **CART (Classification And Regression Tree)**.

![[Pasted image 20240221191038.png]]
где pk​ - доля объектов k-го класса в вершине,  а K - число классов в задаче.
![[Pasted image 20240221191113.png]]
## Pruning
### _Cost-Complexity pruning_
> добавляем регуляризацию к функционалу ошибки дерева.
![[Pasted image 20240221194820.png]]

где ∣T∣ - число вершин в дереве T, α - коэффициент регуляризации.

При оптимизации такого функционала в итоговом дереве штрафуется число вершин, то есть построенное дерево будет содержать меньше вершин, чем без регуляризации - значит, будет проще и менее переобученным.
### Визуализация 
```
import dtreeviz  

viz_model = dtreeviz.model(model, Xtrain, ytrain,
feature_names=Xtrain.columns)  

viz_model.view(fancy = False, scale = 2)
```



____
### Zero-Links
[[Classic ML]]

____
### Links