Tags: #
____
до 2017 года (выхода оригинальной статьи про архитектуру «трансформер») было использование рекуррентных нейронных сетей, или RNN. Однако у такого подхода есть несколько известных минусов:

-  RNN содержат всю информацию о последовательности в скрытом состоянии, которое обновляется с каждым шагом. Если модели необходимо «вспомнить» что-то, что было сотни шагов назад, то эту информацию необходимо хранить внутри скрытого состояния и не заменять чем-то новым. Следовательно, придется иметь либо очень большое скрытое состояние, либо мириться с потерей информации.
- обучение рекуррентных сетей сложно распараллелить: чтобы получить скрытое состояние RNN-слоя для шага i+1, вам необходимо вычислить состояние для шага i. Таким образом, обработка батча примеров длиной 1000 должна потребовать 1000 последовательных операций, что занимает много времени и не очень эффективно работает на GPU, созданных для параллельных вычислений.
Обе этих проблемы затрудняют применение RNN к по-настоящему длинным последовательностям: даже если вы дождетесь конца обучения, ваша модель по своей конструкции будет так или иначе терять информацию о том, что было в начале текста.

## Слой внимания
Первая часть transformer-блока — это слой self-attention. От обычного внимания его отличает то, что выходом являются новые представления для элементов той же последовательности, что мы подали на вход, причем каждый элемент этой последовательности напрямую взаимодействует с каждым.

## FNN
Вторая часть трансформерного блока называется feed-forward network (FFN) и представляет собой два обычных полносвязных слоя, применяемых независимо к каждому элементу входной последовательности. В последних архитектурах размер промежуточного представления (то есть выхода первого слоя) бывает весьма большим — в 4 раза больше выходов блока.

Промежуточные активации act в FFN бывают разными: начиналось всё с широко известной ReLU, но в какой-то момент сообщество перешло на [GELU (Gaussian Error Linear Unit)](https://arxiv.org/abs/1606.08415v4) с формулой xΦ(x), где Φ — функция распределения стандартной нормальной случайной величины.

## позиционные эмбеддинги
Это вспомогательные представления, которые прибавляются к обычным эмбеддингам токенов входной последовательности и позволяют слоям внимания различать одинаковые токены на разных местах.
напрямую учесть тот факт, что нам важны не абсолютные позиции токенов, а расстояние между ними, и обучать [_относительные_](https://arxiv.org/abs/1803.02155) позиционные представления: подобный подход заметно улучшает качество на чувствительных к порядку слов задачах, а его более современные [модификации](https://arxiv.org/abs/2108.12409) регулярно используются в самых мощных моделях.
____
### Zero-Links

____
### Links
https://education.yandex.ru/handbook/ml/article/transformery
