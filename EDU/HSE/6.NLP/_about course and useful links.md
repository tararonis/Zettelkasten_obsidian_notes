Tags: #
____
### **Course materials**

• [Syllabus (Notion)](https://smoggy-gecko-d72.notion.site/NLP-964d0f37013544db84861e0538fe6ff4)

• [Github](https://github.com/greedisneutral/NLP-course)

• [HSE Wiki](http://wiki.cs.hse.ru/%D0%90%D0%B2%D1%82%D0%BE%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0_24/25)

### **Useful sources**

• [NLP Course For You](https://lena-voita.github.io/nlp_course.html)

• [YSDA NLP Course](https://github.com/yandexdataschool/nlp_course)

• [CS224n](https://web.stanford.edu/class/cs224n/)
### Grade policy

- 70% (homework) + 30% (exam)
- 70% (homework) + 30% (exam)
#### Homeworks

- Mandatory: 
(30%) Week 2. Training embeddings using the fasttext library, implementation of a real search engine for embedding-response upon request in a vector database.
(20%) Week 4. Fine-tuning BERT on your own data. (20%) Week 5: Fine tuning LLM using PEFT. 

- Optional: 
(15%) Week 6. Fine-tuning your own model using the TRL library. 
(15%) Week 7. Implementation of Round-to-Nearest (RTN), Generalized Post-Training Quantization (GPTQ)


____
### Zero-Links

____
### Links