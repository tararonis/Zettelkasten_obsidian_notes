Tags: #transformers #hse
____
### shortly
**«ванильная» архитектура, механизм внимания (attention), мотивация и импликации.**
— Концепция внимания
— Трансформеры
	— Self-attention
	— Multi-Head Self-attention  
	— Positional Encoding
— BERT




____
### Zero-Links

____
### Links