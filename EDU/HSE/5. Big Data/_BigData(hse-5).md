Tags: #
____
# Описание

# ==Аннотация== 

**_Для чего нужны платформы данных?_** Как бы это очевидно ни звучало, они нужны для принятия решений на основе данных. У бизнеса может быть бесконечное количество запросов: как изменить цену товаров, в каком месте разместить магазин, кому показать рекламу, куда сдвинуть кнопку на сайте и т.д. Эти задачи могут решаться с помощью аналитики, построения дашбордов, сбора метрик, построения ML-моделей и т.д. Само по себе решение таких задач нетривиально, однако, это лишь верхушка айсберга.

Прежде, чем проводить какую-либо аналитику, данные нужно найти, загрузить в хранилище, проверить их качество, агрегировать, и на любом из этапов может возникнуть огромное количество проблем. Здесь и появляются **платформы данных**, которые предоставляют инфраструктуру и инструменты для загрузки, обработки, проверки и анализа данных, что в конечном счете значительно упрощает и ускоряет решение любых задач, связанных с данными.

# ==Цель освоения дисциплины==

- Понимать принципы работы и структуры платформ данных
- Изучить архитектуры и компоненты систем обработки больших данных на примере экосистемы Hadoop, DataLake, DataWarehouse
- Научиться запускать, развертывать и управлять кластерами и платформами данных
- Освоить инструменты и методы работы с большими данными
- Понимать, как управлять качеством данных - Изучить современные инструменты бизнес-аналитики 

# ==Планируемые результаты обучения==

- Студенты будут знать ключевые компоненты и архитектуру Hadoop
- Студенты смогут развернуть кластер Hadoop, выполнять базовые операции с HDFS и запускать задания MapReduce
- Студенты будут знать, как взаимодействовать с компонентами Hadoop и использовать проекты экосистемы для решения практических задач
- Студенты смогут различать и описывать различные способы организации хранилищ данных: DataLake, DataWarehouse, Lakehouse
- Студенты будут знать архитектуру и возможности Greenplum, Clickhouse, Kafka и Spark, а также смогут выполнять вычисления на кластере Hadoop с использованием Spark.
- Студенты смогут использовать инструменты Airflow, NiFi, Prefect и dbt для организации ETL-процессов.
- Студенты смогут объяснить и применить принципы управления качеством данных (DQ) и управления данными (DatGov).
- Студенты будут знать современные инструменты бизнес-аналитики (BI) и смогут применять их для анализа и визуализации данных

# ==Пререквизиты==

- Базовое владение Python
- Базовое владение SQL
- Знание основ анализа данных и машинного обучения

# ==Содержание учебной дисциплины==

- Введение в обработку больших данных и Hadoop
- Обработка данных в Hadoop
- Аналитика и обработка данных с использованием Hive
- Apache Spark и его применение
- Инструменты организации ETL-процессов
- Технологии хранения и обработки больших данных
- Управление качеством данных и бизнес-аналитика

# ==Элементы контроля==

- Практическое задание №1: Развертывание Hadoop
- Практическое задание №2: Развертывание YARN и выполнение распределенного MapReduce
- Практическое задание №3: Развертывание Hive 
- Практическое задание №4: Развертывание кластера Spark
- Практическое задание №5: Развертывание Airflow, реализация ETL процесса с его применением
- Практическое задание №6: скоро появится информация  
    
- Практическое задание №7: скоро появится информация


____
### Zero-Links

____
### Links
https://my.mts-link.ru/course-info/827093